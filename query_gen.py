import os
from dotenv import load_dotenv
from openai import AzureOpenAI

from azure.search.documents import SearchClient
from azure.search.documents.models import VectorizedQuery
from azure.core.credentials import AzureKeyCredential

import streamlit as st


load_dotenv()
AZURE_OPENAI_KEY = os.getenv("AZURE_OPENAI_KEY")
AZURE_OPENAI_ENDPOINT = os.getenv("AZURE_OPENAI_ENDPOINT")
AZURE_DEPLOYMENT_MODEL = os.getenv("OPENAI_EMBEDDING_DEPLOYMENT_NAME")

AZURE_SEARCH_ENDPOINT = os.getenv("AZURE_SEARCH_ENDPOINT")
AZURE_SEARCH_API_KEY = os.getenv("AZURE_SEARCH_KEY")
AZURE_INDEX_NAME = os.getenv("INDEX_NAME")


openai_client = AzureOpenAI(
    api_version="2024-12-01-preview",
    api_key=AZURE_OPENAI_KEY, 
    azure_endpoint=AZURE_OPENAI_ENDPOINT)


search_client = SearchClient(
    endpoint=AZURE_SEARCH_ENDPOINT,
    index_name=AZURE_INDEX_NAME,
    credential=AzureKeyCredential(AZURE_SEARCH_API_KEY)
)

# -------------------------------
# ë²¡í„° ì„ë² ë”© ìƒì„± í•¨ìˆ˜
# -------------------------------
def get_embedding(text: str):
    resp = openai_client.embeddings.create(
        model=AZURE_DEPLOYMENT_MODEL,
        input=text
    )
    return resp.data[0].embedding

# -------------------------------
# RAG ê¸°ë°˜ SQL ìƒì„± í•¨ìˆ˜
# -------------------------------
def generate_sql_from_rag(user_input: str, top_k: int = 3) -> str:
    # 1. ì¿¼ë¦¬ ì„ë² ë”© ìƒì„±
    query_vector = get_embedding(user_input)
    
    # 2. Azure Search ë²¡í„° ê²€ìƒ‰
    vector_query = VectorizedQuery(
            vector=query_vector,
            k_nearest_neighbors=5,
            fields="embedding",
            kind="vector",
            exhaustive=True
        )
    
    results = search_client.search(vector_queries=[vector_query])
    
    # 3. ê²€ìƒ‰ëœ í…Œì´ë¸” êµ¬ì¡° í•©ì¹˜ê¸°
    retrieved_context = " ".join([doc["schema_text"] for doc in results])

    # 4. LLMì— SQL ìƒì„± ìš”ì²­
    response = openai_client.chat.completions.create(
        model="gpt-4.1-mini",
        messages=[
            {"role": "system", "content": "ë‹¹ì‹ ì€ SQL ì¿¼ë¦¬ ìƒì„± ì „ë¬¸ê°€ì…ë‹ˆë‹¤."},
            {"role": "user", "content": f"ì‚¬ìš©ì ìš”ì²­: {user_input}\ní…Œì´ë¸” êµ¬ì¡°:\n{retrieved_context}\nSQL ì¿¼ë¦¬:"}
        ],
        temperature=0,
        top_p=1,
        max_tokens=300
    )
    sql_query = response.choices[0].message.content
    return sql_query.strip()



# -------------------------------
# Streamlit Chat UI
# -------------------------------
st.title("ğŸ—¨ï¸ RAG ê¸°ë°˜ SQL ìƒì„±ê¸°")

# ì„¸ì…˜ ì´ˆê¸°í™”
if "messages" not in st.session_state:
    st.session_state.messages = []

# ì´ì „ ëŒ€í™” ë³´ì—¬ì£¼ê¸°
for msg in st.session_state.messages:
    with st.chat_message(msg["role"]):
        if msg["role"] == "assistant":
            st.code(msg["content"], language="sql")
        else:
            st.markdown(msg["content"])

# ì‚¬ìš©ì ì…ë ¥ (ì±„íŒ… ì…ë ¥ì°½)
if prompt := st.chat_input("SQL ìš”ì²­ì„ ì…ë ¥í•˜ì„¸ìš” (ì˜ˆ: ê³„ì•½ë²ˆí˜¸ë¡œ ê³„ì•½ì¼ì ê²€ìƒ‰)"):
    # ì‚¬ìš©ì ë©”ì‹œì§€ ì¶”ê°€
    st.session_state.messages.append({"role": "user", "content": prompt})
    with st.chat_message("user"):
        st.markdown(prompt)

    # ëŒ€ê¸° UI ìë¦¬ í‘œì‹œì ìƒì„±
    with st.chat_message("assistant"):
        placeholder = st.empty()
        placeholder.markdown(
            """
            <div style="text-align:center; padding: 20px;">
                <img src="https://i.gifer.com/YCZH.gif" width="200">  <!-- í¬ê¸° í‚¤ì›€ -->
                <p style="font-size:18px;">ì¿¼ë¦¬ ìƒì„± ì¤‘... ì ì‹œë§Œ ê¸°ë‹¤ë ¤ ì£¼ì„¸ìš” ğŸ±â€ğŸ’»</p>
            </div>
            """,
            unsafe_allow_html=True
        )

    # ì‹¤ì œ ì¿¼ë¦¬ ìƒì„±
    sql = generate_sql_from_rag(prompt)

    # ëŒ€ê¸° UI ì œê±°í•˜ê³  ìµœì¢… SQL í‘œì‹œ
    placeholder.code(sql, language="sql")

    # ì„¸ì…˜ì— ìµœì¢… SQL ì—…ë°ì´íŠ¸
    st.session_state.messages.append({"role": "assistant", "content": sql})